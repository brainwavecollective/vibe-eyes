# Emotional Affect Engine

The emotion engine is a real-time affective computing pipeline that extracts a continuous emotional state from conversational text. It is designed to be fast enough for live streaming dialogue, stable enough to avoid flickering, and expressive enough to drive visual output. Stability emerges from blending multiple extractions and additional smoothing of transitions performed on the embedded device.

---

## Emotional Model: VAD+CC

The engine extends the VAD framework with two additional dimensions:

| Dimension  | Range | Meaning |
|------------|-------|---------|
| Valence    | 0–1   | Negative ↔ Positive emotional tone |
| Arousal    | 0–1   | Calm ↔ Energized intensity |
| Dominance  | 0–1   | Passive ↔ Powerful presence |
| Complexity | 0–1   | Lower ↔ Higher readability grade |
| Coherence  | 0–1   | Lower ↔ Higher semantic similarity between adjacent sentences |

Complexity is derived from normalized Flesch–Kincaid grade level.  
Coherence is derived from cosine similarity between adjacent sentence embeddings.

These dimensions capture structural properties of language in addition to affective tone. Two passages may share similar VAD values but differ in Complexity or Coherence due to readability level or semantic continuity.

---

## Architecture: Two Loops

Emotional state is derived from two concurrent loops operating at different timescales.

### Fast Loop (per-sentence, real-time)

Scores each sentence as it arrives using `extractor.py`:

- VAD scores are derived from the NRC-VAD Lexicon (54,801 terms) using intensity-weighted aggregation. Words further from neutral (0.5) contribute more heavily using a nonlinear weighting function.
- Words not found in the lexicon contribute a neutral value (0.5, 0.5, 0.5) with minimal weight.
- Complexity is computed from Flesch–Kincaid grade level and normalized to the range [0,1].
- Coherence is computed as the mean cosine similarity between adjacent sentence embeddings generated by `all-MiniLM-L6-v2`.

The result is a VAD+CC burst vector representing the current sentence.

### Slow Loop (periodic, non-blocking)

Runs `ollama_anchor.py` at longer intervals using a local Ollama model (`nemotron-mini:4b-instruct-q5_K_M` by default):

- A sliding window of recent transcript context is passed to the model with a structured prompt requesting five numeric values.
- Output is constrained to subtle shifts within the range 0.2–0.8.
- If Ollama is unavailable, the system operates without baseline correction.
- The slow loop runs independently and does not block the fast loop.

The slow loop provides a longer-timescale baseline estimate.

---

## Blending

Fast bursts and slow baseline updates are combined in `blender.py` using an attack–hold–decay mechanism.

When a burst arrives:

1. The burst is displayed immediately at full strength.
2. It is held briefly.
3. It then decays toward the current baseline.
4. The baseline is nudged toward the burst by a configurable influence factor.

Baseline updates from the slow loop use a stronger influence value.

Decay occurs incrementally on each tick (typically ~10 Hz).

The `influence` parameter controls how strongly each burst shifts the baseline:

| Value | Behavior |
|-------|----------|
| 0.0   | No baseline shift |
| ~0.2  | Gradual drift |
| ~0.7  | Strong shift |
| 1.0   | Immediate baseline replacement |

---

## Cinematic Amplification

Raw VAD+CC scores from conversational text tend to cluster near neutral (0.4–0.7). Amplification increases contrast for visual output.

`amplifier.py` performs two steps:

### 1. Radial Amplification (passion)

Valence, Arousal, and Dominance are amplified nonlinearly around neutral (0.5). Dimensions further from neutral are amplified more strongly. Complexity and Coherence are not radially amplified.

`passion` controls the strength of this nonlinear gain.

### 2. Cinematic Snapping (drama)

The amplified vector is optionally interpolated toward the nearest exemplar in the cinematic mood dataset (`danielritchie/cinematic-mood-palette`).

Nearest exemplar search uses weighted Euclidean distance with dimension weights:

- Valence: 3.0  
- Arousal: 3.0  
- Dominance: 1.2  
- Complexity: 0.3  
- Coherence: 0.3  

Interpolation is linear:

```
result = values + drama × (target - values)
```

`drama` controls interpolation toward the cinematic exemplar:

| Value | Behavior |
|-------|----------|
| 0.0   | No cinematic shift |
| 0.5   | Partial interpolation |
| 1.0   | Full snap to exemplar |

---

## Output

The engine outputs a VAD+CC vector as a space-delimited message:

```
VIBE 0.7 0.8 0.6 0.5 0.9
```

Values are floats in [0, 1] in the order:

Valence, Arousal, Dominance, Complexity, Coherence.

The output interface is generic and does not depend on the downstream consumer.

---

## Engine Components

| File | Role |
|------|------|
| `extractor.py` | Fast loop — VAD+CC extraction from text |
| `amplifier.py` | Radial amplification and cinematic snapping |
| `blender.py` | Attack–hold–decay blending and baseline shifting |
| `ollama_anchor.py` | Slow loop — numeric baseline extraction via Ollama |
| `output.py` | Serial output to embedded device |
| `server.py` | FastAPI server coordinating components |

